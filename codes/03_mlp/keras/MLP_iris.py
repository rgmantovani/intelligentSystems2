# -*- coding: utf-8 -*-
"""SI 2 - MLP_iris.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bd4_L1J2gNq4l_yareJYSZOU3a3Tv3_i
"""

# numpy é usada para manipulação de arrays numéricos
import numpy as np

#pandas é usados para manipulação de data frames
import pandas as pd

# Tensorflow é a implementação padrão dos modelos de DL, ele é base do Keras
from tensorflow import keras
from keras.utils import to_categorical

# objetos do keras para criar arquiteturas de DL
from keras.models import Sequential
from keras.layers import Dense

# para geração de gráficos
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.model_selection import train_test_split

iris = datasets.load_iris()
print(iris)

df = pd.DataFrame(iris['data'], columns = iris['feature_names'])
df['target'] = pd.Series(iris['target'], name = 'target_values')
df.head()

# Define the features and target
X = df.loc[:, df.columns != 'target']
Y = df['target']

# Split the data into training and testing sets stratified by multiple columns
X_train, X_test, y_train, y_test = train_test_split(
    X,
    Y,
    test_size=0.3,
    random_state=42,
    stratify=Y
)

# Dimensões do dataset [treino e teste]
# Treino = (x_train, y_train)
print("* x_train: " + str(type(X_train)) + " com " + str(X_train.shape))
print("* y_train: " + str(type(y_train)) + " com " + str(y_train.shape))

# Teste = (x_test, y_test)
print("* x_test:  " + str(type(X_test)) + " com " + str(X_test.shape))
print("* y_test:  " + str(type(y_test)) + " com " + str(y_test.shape))

#  Create categorical labels

train_labels = to_categorical(y_train)
test_labels = to_categorical(y_test)

# Verificando as dimensões dos conjuntos
print('X_train:', X_train.shape)
print('y_train:', y_train.shape)
print('X_test:', X_test.shape)
print('y_test:', y_test.shape)

# Vendo o dataset como um data frame
pd.DataFrame(X_train)

# Definindo uma MLP

#MLP é um modelo de rede neura sequencial
mlpModel = Sequential()
mlpModel.add(Dense(10, activation = "sigmoid", input_shape=(4, )))
mlpModel.add(Dense(3, activation = "sigmoid"))

# imprime o modelo, para verificarmos a arquitetura
mlpModel.summary()

# Aqui especificamos como será o processo de treinamento do modelo
# Nós vamos usar:
#    - SGD: algoritmo de gradiente descendente para treinar a rede
#    - binary crossentropy: entropia binária cruzada como medida de erro (loss),
#         que vai ser minimizada entre as épocas
#.   - accuracy: acurácia do modelo em cada época
mlpModel.compile(
    loss='binary_crossentropy',
    optimizer='SGD',
    metrics=['accuracy']
)

# Esse é o setup experimental para execução do treinamento da rede
# o método fit chama o treinamento da rede neural
#     - X_train: é o conjunto de treinamento
#     - y_train: são os rótulos do conjunto de treinamento
#     - epochs: quantidade de épocas que a rede irá treinar
#     - batch_size: quantidade de exemplos treinados em lote, para gerar um ajuste de pesos

history = mlpModel.fit(
    X_train, train_labels,
    epochs = 50,
    batch_size = 2,
    verbose=2
)

#Visualizando as curvas de erro e acurácia
plt.figure(figsize=(5,3))
plt.plot(history.epoch,history.history['loss'])
plt.title('training loss')

plt.figure(figsize=(5,3))
plt.plot(history.epoch,history.history['accuracy'])
plt.title('training accuracy');

# Avaliando o modelo treinado no conjunto de testes
scores = mlpModel.evaluate(X_test, test_labels, verbose=2)
print("%s: %.2f%%" % (mlpModel.metrics_names[1], scores[1]*100))