# -*- coding: utf-8 -*-
"""Simple RNN - Single Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZcM0iT8nE-G7N-y9yEpG6jH-abqRs8XF

# Uso de Deep Learning para Séries Temporais (LSTMs)

**Disciplina**: Sistemas Inteligentes 2

Graduação em Engenharia de Computação - Universidade Tecnologica Federal do Paraná

**Aula 09**: RNNs e LSTMs

**Exemplo 01**: Executando Redes Recorrentes em Séries Temporais
---
"""

# Commented out IPython magic to ensure Python compatibility.
# importando as bibliotecas necessárias para rodar o script

# manipulação de números de ponto flutuante
import numpy as np

# manipulação de redes neurais (keras)
import tensorflow as tf
from tensorflow import keras
from keras.layers import Flatten, Dense, LSTM, SimpleRNN
from keras.models import Sequential
from keras.losses import mean_squared_error

# plotagem de gráficos via matplotlib
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes',  labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

"""# 1 - Gerar datasets e plots de séries temporais artificiais"""

# -------------------------------------------------------------------------------
# Cria uma série temporal univariada. A função cria quantas séries pedirmos,
# com base no parâmetro ``batch_size``. Cada série tem tamanho = n_steps
# -------------------------------------------------------------------------------
def generate_time_series(batch_size, n_steps):
  freq1, freq2, offset1, offset2 = np.random.rand(4, batch_size, 1)
  time = np.linspace(0, 1, n_steps)
  series =  0.5 * np.sin((time - offset1) * (freq1 * 10 + 10)) # onda 1
  series += 0.2 * np.sin((time - offset2) * (freq2 * 20 + 20)) # onda 2
  series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + ruído
  return (series[...,np.newaxis].astype(np.float32))

# seed para reprodutibilidade
keras.utils.set_random_seed(42)

# gerar 10k séries de tamanho = 51
n_steps = 50
series = generate_time_series(batch_size = 10000, n_steps=(n_steps+1))

# dividir os dados em treino/validação/teste
X_train, y_train = series[:7000, :n_steps], series[:7000, -1]
X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]
X_test, y_test   = series[9000:, :n_steps], series[9000:, -1]

# Verificando o formato dos dados
X_train.shape, y_train.shape

"""### **Plotando Séries Temporais**

Função para realizar o plot de uma série temporal, e podermos ver como são os sinais gerados pela nossa função geradora de dados artificiais
"""

def plot_series(series, y=None, y_pred=None, x_label="$t$", y_label="$x(t)$", legend=True):
    plt.plot(series, ".-")
    if y is not None:
        plt.plot(n_steps, y, "bo", label="Target")
    if y_pred is not None:
        plt.plot(n_steps, y_pred, "rx", markersize=10, label="Prediction")
    plt.grid(True)
    if x_label:
        plt.xlabel(x_label, fontsize=16)
    if y_label:
        plt.ylabel(y_label, fontsize=16, rotation=0)
    plt.hlines(0, 0, 100, linewidth=1)
    plt.axis([0, n_steps + 1, -1, 1])
    if legend and (y or y_pred):
        plt.legend(fontsize=14, loc="upper left")

# Visualizando algumas das séries que foram geradas pela nossa função
fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4))
for col in range(3):
    plt.sca(axes[col])
    plot_series(X_valid[col, :, 0], y_valid[col, 0],
                y_label=("$x(t)$" if col==0 else None),
                legend=(col == 0))
plt.show()

# função para plotar as curvas dos erros de treinamento e validação
def plot_learning_curves(loss, val_loss, max_y=0.2):
    plt.plot(np.arange(len(loss)) + 0.5, loss, "b.-", label="Training loss")
    plt.plot(np.arange(len(val_loss)) + 1, val_loss, "r.-", label="Validation loss")
    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))
    plt.axis([1, 20, 0, max_y])
    plt.legend(fontsize=14)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.grid(True)

"""# 2 - Gerando alguns Baselines Simples

---
## 2.1) predições Naive: prediz/repete apenas o último valor observado
---
"""

naive_y_pred = X_valid[:, -1]
naiveForecasting = np.mean(mean_squared_error(y_valid, naive_y_pred))
print(naiveForecasting)

# Visualizando a primeira série temporal e valor predito pelo modelo Nayve
plot_series(X_valid[0, :, 0], y_valid[0, 0], naive_y_pred[0, 0])
plt.show()

"""---
## 2.2) Baseline linear
---

"""

# seed para reprodutibilidade
keras.utils.set_random_seed(42)

# definindo um modelo linear
linearModel = Sequential([
        Flatten(input_shape = [50,1]),
        Dense(1)
])
linearModel.summary()

# Treinando o modelo linear
linearModel.compile(optimizer = "adam", loss = "mse")
linearHistory = linearModel.fit(
    x = X_train,
    y = y_train,
    epochs = 20,
    validation_data = (X_valid, y_valid)
)

# Avaliando o modelo linear no conjunto de validação
linearError = linearModel.evaluate(X_valid, y_valid)
linearError

# Visualizando as curvas de aprendizado do modelo linear
plot_learning_curves(linearHistory.history["loss"], linearHistory.history["val_loss"], max_y = 0.075)
plt.show()

# Computando a proxima predição e visualizando no gráfico da série
linear_y_pred = linearModel.predict(X_valid)
plot_series(X_valid[0, :, 0], y_valid[0, 0], linear_y_pred[0, 0])
plt.show()

"""# 3 - Gerando Redes Recorrentes

---
## 3.1 - Rede recorrente simples
---
"""

# seed para reprodutibilidade
keras.utils.set_random_seed(42)

# modelo simples de RNN
simpleRNNModel = Sequential([
    SimpleRNN(1, input_shape=[None, 1])
])

# treinando a rede
optimizer = keras.optimizers.Adam(learning_rate=0.005)
simpleRNNModel.compile(loss="mse", optimizer=optimizer)
simpleRRNhistory = simpleRNNModel.fit(
    x = X_train,
    y = y_train,
    epochs = 20,
    validation_data=(X_valid, y_valid)
)

# avaliar o modelo simples de RNN
simpleRNNError = simpleRNNModel.evaluate(X_valid, y_valid)
simpleRNNError

# visualizando os erros
plot_learning_curves(simpleRRNhistory.history["loss"], simpleRRNhistory.history["val_loss"], max_y = 0.1)
plt.show()

# Computando a proxima predição e visualizando no gráfico da série
simple_y_pred = simpleRNNModel.predict(X_valid)
plot_series(X_valid[0, :, 0], y_valid[0, 0], simple_y_pred[0, 0])

plt.show()

"""---
## 3.2 - Deep RNNs
---
"""

# seed para reprodutibilidade
keras.utils.set_random_seed(42)

# modelo simples de RNN
deepRNNmodel = Sequential([
    SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    SimpleRNN(20, return_sequences=True),
    SimpleRNN(1)
])

# treinando a rede
deepRNNmodel.compile(loss="mse", optimizer="adam")
deepRNNhistory = deepRNNmodel.fit(
    x = X_train,
    y = y_train,
    epochs = 20,
    validation_data=(X_valid, y_valid)
)

# avaliando o modelo em termos de MSE
deepRNNError = deepRNNmodel.evaluate(X_valid, y_valid)
deepRNNError

# Visualizando as curvas de aprendizado do deep RNN
plot_learning_curves(deepRNNhistory.history["loss"], deepRNNhistory.history["val_loss"], max_y = 0.04)
plt.show()

# Visualizando a predição
deepRNN_y_pred = deepRNNmodel.predict(X_valid)
plot_series(X_valid[0, :, 0], y_valid[0, 0], deepRNN_y_pred[0, 0])
plt.show()

"""---
## 3.3 - Segunda RNN simples
---

Aapenas a segunda camada (oculta) retorna o valor na retroalimentação:
"""

# seed para reprodutibilidade
keras.utils.set_random_seed(42)

# modelo simples de RNN
deepRNNmodel2 = Sequential([
    SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), # essa camada retroalimenta a rede
    SimpleRNN(20),                                               # essa camada não
    Dense(1)
])

# treinando a rede
deepRNNmodel2.compile(loss="mse", optimizer="adam")
deepRNNmodelhistory2 = deepRNNmodel2.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))

deepRNN2Error = deepRNNmodel2.evaluate(X_valid, y_valid)
deepRNN2Error

plot_learning_curves(deepRNNmodelhistory2.history["loss"], deepRNNmodelhistory2.history["val_loss"], max_y = 0.02)
plt.show()

deepRNN2_y_pred = deepRNNmodel2.predict(X_valid)
plot_series(X_valid[0, :, 0], y_valid[0, 0], deepRNN2_y_pred[0, 0])
plt.show()

"""---
## 3.4 - LSTM
---
"""

# seed para reprodutibilidade
keras.utils.set_random_seed(42)

# modelo LSTM
lstmModel = Sequential([
    LSTM(50, return_sequences=True, input_shape=[None, 1]),
    Dense(1)
])

# treinando a rede
lstmModel.compile(loss="mse", optimizer="adam")
lstmHistory = lstmModel.fit(
    x = X_train,
    y = y_train,
    epochs = 20,
    validation_data=(X_valid, y_valid)
)

lstmError = lstmModel.evaluate(X_valid, y_valid)
lstmError

plot_learning_curves(lstmHistory.history["loss"], lstmHistory.history["val_loss"], max_y = 0.15)
plt.show()

lstm_y_pred = lstmModel.predict(X_valid)
plot_series(X_valid[0, :, 0], y_valid[0, 0], lstm_y_pred[0, 0])
plt.show()

lstm_y_pred[0][0][0]

"""---
## 3.5 - LSTM com mais unidades e camadas
---
"""

# seed para reprodutibilidade
keras.utils.set_random_seed(42)

deepLstmModel = Sequential([
    LSTM(50, activation='relu', return_sequences=True, input_shape=[None, 1]),
    LSTM(50, activation='relu'),
    Dense(1)
])

deepLstmModel.compile(loss="mse", optimizer="adam")
deepLstmHistory = deepLstmModel.fit(
    x = X_train,
    y = y_train,
    epochs = 20,
    validation_data=(X_valid, y_valid)
)

deepLstmError = deepLstmModel.evaluate(X_valid, y_valid)
deepLstmError

plot_learning_curves(deepLstmHistory.history["loss"], deepLstmHistory.history["val_loss"], max_y = 0.006)
plt.show()

deepLSTM_y_pred = deepLstmModel.predict(X_valid)
plot_series(X_valid[0, :, 0], y_valid[0, 0], deepLSTM_y_pred[0, 0])
plt.show()

"""---
# 4 - Comparar predições
---
"""

# def plot_series(series, y=None, y_pred=None, x_label="$t$", y_label="$x(t)$", legend=True):
plt.plot(X_valid[0, :, 0], ".-")
plt.plot(n_steps, y_valid[0, 0], "bo", label="Target")
plt.plot(n_steps, naive_y_pred[0, 0], "rx", markersize=10, label="Naive")
plt.plot(n_steps, linear_y_pred[0, 0], "g*", markersize=10, label="Linear")
plt.plot(n_steps, simple_y_pred[0, 0], "cv", markersize=10, label="Simple RNN")
plt.plot(n_steps, deepRNN_y_pred[0, 0], "m^", markersize=10, label="Deep RNN")
plt.plot(n_steps, deepRNN2_y_pred[0, 0], "ys", markersize=10, label="Deep RNN 2")
plt.plot(n_steps, lstm_y_pred[0][0][0], "kp", markersize=10, label="LSTM")
plt.plot(n_steps, deepLSTM_y_pred[0, 0], "rP", markersize=10, label="Deep LSTM")

plt.grid(True)
plt.xlabel("$t$", fontsize=16)
plt.ylabel("$x(t)$", fontsize=16, rotation=0)
plt.hlines(0, 0, 100, linewidth=1)
plt.axis([0, n_steps + 1, -1, 1])
plt.legend(fontsize=14, loc=3)

import pandas as pd

predictions = pd.DataFrame([naive_y_pred[0, 0], linear_y_pred[0, 0], simple_y_pred[0, 0],
               deepRNN_y_pred[0, 0], deepRNN2_y_pred[0, 0], lstm_y_pred[0][0][0], deepLSTM_y_pred[0, 0]])

errors = pd.DataFrame([
    naiveForecasting, #naive
    linearError,
    simpleRNNError,
    deepRNNError,
    deepRNN2Error,
    lstmError,
    deepLstmError
])

methods = pd.DataFrame(["Naive", "Linear", "Simple RNN", "Deep RNN", "Deep RNN 2", "LSTM", "Deep LSTM"])

df = pd.concat([predictions, errors, methods], axis = 1)
df.columns = ['Prediction', 'Error', 'Methods']
df

df['Prediction'] = df['Prediction'].round(decimals=3)
df['Error'] = df['Error'].round(decimals=3)
df